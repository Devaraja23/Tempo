{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b6a0e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Setup and imports\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.ao.quantization as tq\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106679fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Data paths and attribute table\n",
    "base_dir = Path(r\"D:\\rtx\")\n",
    "img_dir = base_dir / \"img_align_celeba\" / \"img_align_celeba\"\n",
    "attr_file = base_dir / \"list_attr_celeba.csv\"\n",
    "\n",
    "subset_dir = base_dir / \"subset_celeba\"\n",
    "subset_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Load attributes CSV\n",
    "df = pd.read_csv(attr_file)\n",
    "df.set_index(\"image_id\", inplace=True)\n",
    "\n",
    "print(\"Total images in CelebA:\", len(df))\n",
    "\n",
    "# Select a subset (e.g., first 10,000 rows); adjust as needed\n",
    "subset_n = 10000\n",
    "filtered_df = df.head(subset_n).copy()\n",
    "\n",
    "# Map -1/1 to 0/1\n",
    "filtered_df = (filtered_df + 1) // 2\n",
    "filtered_df = filtered_df.astype(np.int64)\n",
    "\n",
    "print(\"Subset attribute table shape:\", filtered_df.shape)\n",
    "display(filtered_df.head())\n",
    "\n",
    "# Copy images to subset folder if not present\n",
    "copied = 0\n",
    "for img_name in filtered_df.index:\n",
    "    src = img_dir / img_name\n",
    "    dst = subset_dir / img_name\n",
    "    if src.exists() and not dst.exists():\n",
    "        Image.open(src).save(dst)  # preserves modes and avoids shutil permission oddities\n",
    "        copied += 1\n",
    "print(f\" Copied {copied} images to {subset_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938beece",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Dataset and transforms\n",
    "from torchvision import transforms\n",
    "\n",
    "attr_names = list(filtered_df.columns)\n",
    "attr_to_idx = {a:i for i,a in enumerate(attr_names)}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.CenterCrop((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5]),\n",
    "])\n",
    "\n",
    "\n",
    "class CelebASubset(Dataset):\n",
    "    def __init__(self, img_dir: Path, attr_df: pd.DataFrame, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.attr_df = attr_df\n",
    "        self.names = list(attr_df.index)\n",
    "        self.transform = transform\n",
    "        self.attrs = attr_df.values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.names[idx]\n",
    "        path = self.img_dir / name\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform: img = self.transform(img)\n",
    "        attrs = torch.from_numpy(self.attrs[idx])\n",
    "        return img, attrs\n",
    "\n",
    "dataset = CelebASubset(subset_dir, filtered_df, transform)\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "len(dataset), len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f538f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Self-attention and fuseable conv blocks\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(in_ch, in_ch // 8, 1, bias=False)\n",
    "        self.key   = nn.Conv2d(in_ch, in_ch // 8, 1, bias=False)\n",
    "        self.value = nn.Conv2d(in_ch, in_ch, 1, bias=False)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        q = self.query(x).view(b, -1, h*w).permute(0, 2, 1)  # b, hw, cq\n",
    "        k = self.key(x).view(b, -1, h*w)                     # b, cq, hw\n",
    "        attn = torch.softmax(torch.bmm(q, k), dim=-1)        # b, hw, hw\n",
    "        v = self.value(x).view(b, c, h*w)                    # b, c, hw\n",
    "        out = torch.bmm(v, attn.permute(0, 2, 1)).view(b, c, h, w)\n",
    "        return self.gamma * out + x\n",
    "\n",
    "def up_block(cin, cout):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(cin, cout, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(cout),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "def down_block(cin, cout, bn=True):\n",
    "    layers = [nn.Conv2d(cin, cout, 4, 2, 1, bias=not bn)]\n",
    "    if bn: layers.append(nn.BatchNorm2d(cout))\n",
    "    layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class AttrEncoder1x1(nn.Module):\n",
    "    def __init__(self, attr_dim=40, out_ch=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(attr_dim, 128, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, out_ch, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, attrs):  # attrs: (B, attr_dim)\n",
    "        return self.net(attrs.unsqueeze(-1).unsqueeze(-1))  # (B,out_ch,1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8373ac1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Attention AttGAN models (128x128)\n",
    "\n",
    "class AttnAttGANGenerator(nn.Module):\n",
    "    def __init__(self, z_dim=100, attr_dim=40, base=64, img_ch=3):\n",
    "        super().__init__()\n",
    "        self.quant = tq.QuantStub()\n",
    "        self.dequant = tq.DeQuantStub()\n",
    "        self.attr = AttrEncoder1x1(attr_dim, out_ch=128)\n",
    "\n",
    "        # Mix latent+attrs at 1x1\n",
    "        self.mix = nn.Sequential(\n",
    "            nn.Conv2d(z_dim + 128, base*8, 1, bias=False),\n",
    "            nn.BatchNorm2d(base*8),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Upsample chain: 4 -> 8 -> 16 -> 32 -> 64 -> 128\n",
    "        self.up1 = up_block(base*8, base*4)     # 4->8\n",
    "        self.up2 = up_block(base*4, base*2)     # 8->16\n",
    "        self.att16 = SelfAttention(base*2)      # attention at 16x16\n",
    "        self.up3 = up_block(base*2, base)       # 16->32\n",
    "        self.att32 = SelfAttention(base)        # attention at 32x32\n",
    "        self.up4 = up_block(base, base//2)      # 32->64\n",
    "        self.att64 = SelfAttention(base//2)     # attention at 64x64 (new for 128 target)\n",
    "        self.up5 = up_block(base//2, base//4)   # 64->128\n",
    "\n",
    "        # Refine head at 128x128\n",
    "        self.refine = nn.Sequential(\n",
    "            nn.Conv2d(base//4, base//4, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(base//4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base//4, img_ch, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def forward(self, z, attrs):\n",
    "        z = z.unsqueeze(-1).unsqueeze(-1)          # (B,z,1,1)\n",
    "        a = self.attr(attrs)                       # (B,128,1,1)\n",
    "        x = torch.cat([z, a], 1)\n",
    "        x = self.quant(x)\n",
    "        x = self.mix(x)\n",
    "        x = F.interpolate(x, size=(4,4), mode='nearest')\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x); x = self.att16(x)\n",
    "        x = self.up3(x); x = self.att32(x)\n",
    "        x = self.up4(x); x = self.att64(x)\n",
    "        x = self.up5(x)\n",
    "        x = self.refine(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttnAttGANDiscriminator(nn.Module):\n",
    "    def __init__(self, attr_dim=40, img_ch=3, base=64):\n",
    "        super().__init__()\n",
    "        self.quant = tq.QuantStub()\n",
    "        self.dequant = tq.DeQuantStub()\n",
    "\n",
    "        # Attribute encoder (1x1 conv to feature channels)\n",
    "        self.attr = nn.Sequential(\n",
    "            nn.Conv2d(attr_dim, base, 1, bias=False),\n",
    "            nn.BatchNorm2d(base),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        # Downsample chain: 128 -> 64 -> 32 -> 16 -> 8 -> 4\n",
    "        self.conv1 = down_block(img_ch + base, base, bn=False)  # 128->64 (no BN)\n",
    "        self.att64 = SelfAttention(base)                         # attention at 64x64\n",
    "        self.conv2 = down_block(base, base*2)                    # 64->32\n",
    "        self.conv3 = down_block(base*2, base*4)                  # 32->16\n",
    "        self.conv4 = down_block(base*4, base*8)                  # 16->8\n",
    "        self.conv5 = down_block(base*8, base*8)                  # 8->4 (keep channels)\n",
    "\n",
    "        # Heads (logits)\n",
    "        self.adv_head = nn.Conv2d(base*8, 1, 4, 1, 0)\n",
    "        self.attr_head = nn.Conv2d(base*8, attr_dim, 4, 1, 0)\n",
    "\n",
    "    def forward(self, img, attrs):\n",
    "        a = self.attr(attrs.unsqueeze(-1).unsqueeze(-1))\n",
    "        a = a.expand(-1, -1, img.size(2), img.size(3))  # tile to HxW\n",
    "        x = torch.cat([img, a], 1)\n",
    "\n",
    "        x = self.quant(x)\n",
    "        x = self.conv1(x); x = self.att64(x)\n",
    "        x = self.conv2(x); x = self.conv3(x); x = self.conv4(x); x = self.conv5(x)\n",
    "        adv = self.adv_head(x).view(img.size(0), 1)\n",
    "        attr_logits = self.attr_head(x).view(img.size(0), -1)\n",
    "        adv = self.dequant(adv); attr_logits = self.dequant(attr_logits)\n",
    "        return adv, attr_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50003731",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: Fusion utilities for quantization ( for 128x128)\n",
    "\n",
    "def fuse_for_quant_gen(G: AttnAttGANGenerator):\n",
    "    # Fuse Conv+BN+Act in mix\n",
    "    tq.fuse_modules(G.mix, [['0','1','2']], inplace=True)\n",
    "    # Fuse all upsample blocks: up1..up5 (each is ConvT -> BN -> ReLU)\n",
    "    for blk in [G.up1, G.up2, G.up3, G.up4, G.up5]:\n",
    "        tq.fuse_modules(blk, [['0','1','2']], inplace=True)\n",
    "    # Fuse first trio in refine (Conv -> BN -> ReLU); final Conv+Tanh not fused\n",
    "    tq.fuse_modules(G.refine, [['0','1','2']], inplace=True)\n",
    "\n",
    "def fuse_for_quant_disc(D: AttnAttGANDiscriminator):\n",
    "    # Fuse attribute encoder Conv+BN+Act\n",
    "    tq.fuse_modules(D.attr, [['0','1','2']], inplace=True)\n",
    "    # conv1 has no BN (bn=False) -> skip fusion\n",
    "    # Fuse remaining down blocks that have Conv+BN+Act\n",
    "    for name in ['conv2', 'conv3', 'conv4', 'conv5']:\n",
    "        blk = getattr(D, name)\n",
    "        tq.fuse_modules(blk, [['0','1','2']], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c460a795",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7: Instantiate models and optimizers\n",
    "z_dim = 100\n",
    "attr_dim = len(attr_names)\n",
    "\n",
    "G = AttnAttGANGenerator(z_dim=z_dim, attr_dim=attr_dim).to(device)\n",
    "D = AttnAttGANDiscriminator(attr_dim=attr_dim).to(device)\n",
    "\n",
    "lr = 2e-4\n",
    "optimizerG = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerD = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "print(\"Generator params:\", sum(p.numel() for p in G.parameters())/1e6, \"M\")\n",
    "print(\"Discriminator params:\", sum(p.numel() for p in D.parameters())/1e6, \"M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30321c34",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 8: Losses and one-epoch training function\n",
    "bce_logits = nn.BCEWithLogitsLoss()\n",
    "l1 = nn.L1Loss()\n",
    "lambda_attr = 10.0\n",
    "lambda_rec = 50.0\n",
    "\n",
    "def train_one_epoch(G, D, dataloader, optG, optD, device, z_dim=100):\n",
    "    G.train(); D.train()\n",
    "    g_running, d_running = 0.0, 0.0\n",
    "    for imgs, attrs in dataloader:\n",
    "        imgs = imgs.to(device)\n",
    "        attrs = attrs.to(device).float()\n",
    "        bs = imgs.size(0)\n",
    "        z = torch.randn(bs, z_dim, device=device)\n",
    "        attrs_tgt = attrs  # could sample/edit attributes here\n",
    "\n",
    "        # 1 Train D: adv real/fake + attribute on real\n",
    "        optD.zero_grad(set_to_none=True)\n",
    "        real_adv, real_attr_logits = D(imgs, attrs)\n",
    "        d_adv_real = bce_logits(real_adv, torch.ones(bs,1,device=device))\n",
    "        d_attr_real = bce_logits(real_attr_logits, attrs)\n",
    "        fake_imgs = G(z, attrs_tgt).detach()\n",
    "        fake_adv, _ = D(fake_imgs, attrs_tgt)\n",
    "        d_adv_fake = bce_logits(fake_adv, torch.zeros(bs,1,device=device))\n",
    "        d_loss = d_adv_real + d_adv_fake + d_attr_real\n",
    "        d_loss.backward()\n",
    "        optD.step()\n",
    "\n",
    "        # 2 Train G: adv to real + attribute on fake + reconstruction (L1)\n",
    "        optG.zero_grad(set_to_none=True)\n",
    "        gen_imgs = G(z, attrs_tgt)\n",
    "        g_adv, g_attr_logits = D(gen_imgs, attrs_tgt)\n",
    "        g_adv_loss = bce_logits(g_adv, torch.ones(bs,1,device=device))\n",
    "        g_attr_loss = bce_logits(g_attr_logits, attrs_tgt)\n",
    "        rec_imgs = G(z, attrs)  # if identity encoder added, use enc(imgs)\n",
    "        rec_loss = l1(rec_imgs, imgs)\n",
    "        g_loss = g_adv_loss + lambda_attr * g_attr_loss + lambda_rec * rec_loss\n",
    "        g_loss.backward()\n",
    "        optG.step()\n",
    "\n",
    "        g_running += g_loss.item()\n",
    "        d_running += d_loss.item()\n",
    "    return g_running/len(dataloader), d_running/len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96fa57c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 9: FP32 training with checkpointing every 10 epochs\n",
    "ckpt_dir = base_dir / \"checkpoints_attgan\"\n",
    "ckpt_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def save_fp32_checkpoint(epoch, G, D, optG, optD, tag=\"fp32\"):\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"G\": G.state_dict(),\n",
    "        \"D\": D.state_dict(),\n",
    "        \"optG\": optG.state_dict(),\n",
    "        \"optD\": optD.state_dict(),\n",
    "    }\n",
    "    path = ckpt_dir / f\"attgan_{tag}_e{epoch:03d}.pth\"\n",
    "    torch.save(state, path)\n",
    "    print(f\"Saved FP32 checkpoint: {path}\")\n",
    "\n",
    "total_epochs = 30\n",
    "for epoch in range(1, total_epochs+1):\n",
    "    g_loss, d_loss = train_one_epoch(G, D, dataloader, optimizerG, optimizerD, device, z_dim)\n",
    "    print(f\"FP32 Epoch {epoch}/{total_epochs} | G: {g_loss:.4f} | D: {d_loss:.4f}\")\n",
    "    if epoch % 10 == 0:\n",
    "        save_fp32_checkpoint(epoch, G, D, optimizerG, optimizerD, tag=\"fp32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c03e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 9.1: Load FP32 checkpoint and generate samples (no saving)\n",
    "# Adjust checkpoint filename if you want epoch 10 or 20 instead of 30\n",
    "fp32_ckpt_path = base_dir / \"checkpoints_attgan\" / \"attgan_fp32_e030.pth\"\n",
    "assert fp32_ckpt_path.exists(), f\"Missing checkpoint: {fp32_ckpt_path}\"\n",
    "\n",
    "# Recreate models and load FP32 weights\n",
    "G_fp32 = AttnAttGANGenerator(z_dim=z_dim, attr_dim=len(attr_names)).to(device)\n",
    "D_fp32 = AttnAttGANDiscriminator(attr_dim=len(attr_names)).to(device)\n",
    "ckpt = torch.load(fp32_ckpt_path, map_location=device)\n",
    "G_fp32.load_state_dict(ckpt[\"G\"])\n",
    "D_fp32.load_state_dict(ckpt[\"D\"])\n",
    "G_fp32.eval(); D_fp32.eval()\n",
    "\n",
    "# Generate a batch from random attributes\n",
    "with torch.inference_mode():\n",
    "    B = 8\n",
    "    rand_attrs = torch.randint(0, 2, (B, len(attr_names)), device=device, dtype=torch.float32)\n",
    "    z = torch.randn(B, z_dim, device=device)\n",
    "    gen_imgs_fp32 = G_fp32(z, rand_attrs)  # [-1,1], shape (B,3,128,128)\n",
    "\n",
    "# Optional: generate with specific attributes by name\n",
    "name_to_idx = {n:i for i,n in enumerate(attr_names)}\n",
    "def make_attr_vec(pairs):\n",
    "    v = torch.zeros(len(attr_names), device=device)\n",
    "    for k, val in pairs.items():\n",
    "        if k in name_to_idx: v[name_to_idx[k]] = float(val)\n",
    "    return v\n",
    "\n",
    "pairs = {\"Smiling\":1, \"Blond_Hair\":1, \"Male\":0, \"Young\":1}\n",
    "with torch.inference_mode():\n",
    "    attr_vec = make_attr_vec(pairs).unsqueeze(0).repeat(8, 1)\n",
    "    z_sp = torch.randn(8, z_dim, device=device)\n",
    "    gen_imgs_fp32_spec = G_fp32(z_sp, attr_vec)  # [-1,1]\n",
    "\n",
    "print(\"FP32 gen random:\", tuple(gen_imgs_fp32.shape), \"FP32 gen specific:\", tuple(gen_imgs_fp32_spec.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19eb574",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 10: QAT preparation (fuse + prepare_qat)\n",
    "# Choose engine: \"fbgemm\" on x86, \"qnnpack\" on ARM\n",
    "engine = \"fbgemm\" if (os.name == \"nt\" or os.name == \"posix\") else \"fbgemm\"\n",
    "torch.backends.quantized.engine = engine\n",
    "print(\"Quant engine:\", torch.backends.quantized.engine)\n",
    "\n",
    "G.train(); D.train()\n",
    "fuse_for_quant_gen(G); fuse_for_quant_disc(D)\n",
    "\n",
    "G.qconfig = tq.get_default_qat_qconfig(engine)\n",
    "D.qconfig = tq.get_default_qat_qconfig(engine)\n",
    "\n",
    "tq.prepare_qat(G, inplace=True)\n",
    "tq.prepare_qat(D, inplace=True)\n",
    "\n",
    "print(\"QAT observers inserted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c28a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 11: QAT fine-tuning with checkpointing every 10 epochs\n",
    "# Optionally reduce LR for QAT\n",
    "for pg in optimizerG.param_groups: pg[\"lr\"] = min(pg[\"lr\"], 1e-4)\n",
    "for pg in optimizerD.param_groups: pg[\"lr\"] = min(pg[\"lr\"], 1e-4)\n",
    "\n",
    "qat_epochs = 30\n",
    "for ep in range(1, qat_epochs+1):\n",
    "    # Optional: freeze observers/BN late\n",
    "    if ep == 20:\n",
    "        G.apply(torch.ao.quantization.disable_observer)\n",
    "        D.apply(torch.ao.quantization.disable_observer)\n",
    "    if ep == 25:\n",
    "        G.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
    "        D.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
    "\n",
    "    g_loss, d_loss = train_one_epoch(G, D, dataloader, optimizerG, optimizerD, device, z_dim)\n",
    "    print(f\"QAT Epoch {ep}/{qat_epochs} | G: {g_loss:.4f} | D: {d_loss:.4f}\")\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        state = {\n",
    "            \"epoch\": ep,\n",
    "            \"G_qat\": G.state_dict(),\n",
    "            \"D_qat\": D.state_dict(),\n",
    "            \"optG\": optimizerG.state_dict(),\n",
    "            \"optD\": optimizerD.state_dict(),\n",
    "        }\n",
    "        path = ckpt_dir / f\"attgan_qat_e{ep:03d}.pth\"\n",
    "        torch.save(state, path)\n",
    "        print(f\"Saved QAT checkpoint: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b957d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 12: Convert to INT8 for inference\n",
    "G.eval(); D.eval()\n",
    "tq.convert(G, inplace=True)\n",
    "tq.convert(D, inplace=True)\n",
    "print(\"Converted to INT8 (fake-quant removed, quantized ops inserted).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2dedb0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 13 (updated): Load INT8 generator and generate samples (no saving)\n",
    "# If you just ran Cell 12 in this session, you can directly use the in-memory G.\n",
    "# Otherwise, to load from saved INT8 weights, recreate quantized model as below.\n",
    "\n",
    "int8_G_path = base_dir / \"checkpoints_qat\" / \"G_qat_int8.pt\"\n",
    "int8_D_path = base_dir / \"checkpoints_qat\" / \"D_qat_int8.pt\"\n",
    "assert int8_G_path.exists(), f\"Missing quantized G: {int8_G_path}\"\n",
    "\n",
    "# Recreate models, fuse, prepare, convert to quantized graph, then load int8 weights\n",
    "G_int8 = AttnAttGANGenerator(z_dim=z_dim, attr_dim=len(attr_names)).to(device)\n",
    "D_int8 = AttnAttGANDiscriminator(attr_dim=len(attr_names)).to(device)\n",
    "torch.backends.quantized.engine = \"fbgemm\"\n",
    "fuse_for_quant_gen(G_int8); fuse_for_quant_disc(D_int8)\n",
    "G_int8.qconfig = tq.get_default_qat_qconfig(\"fbgemm\")\n",
    "D_int8.qconfig = tq.get_default_qat_qconfig(\"fbgemm\")\n",
    "tq.prepare_qat(G_int8, inplace=True); tq.prepare_qat(D_int8, inplace=True)\n",
    "G_int8.eval(); D_int8.eval()\n",
    "tq.convert(G_int8, inplace=True); tq.convert(D_int8, inplace=True)\n",
    "\n",
    "# Load INT8 state dicts (optional for D)\n",
    "G_int8.load_state_dict(torch.load(int8_G_path, map_location=device), strict=True)\n",
    "\n",
    "# Generate a batch from random attributes\n",
    "with torch.inference_mode():\n",
    "    B = 8\n",
    "    rand_attrs = torch.randint(0, 2, (B, len(attr_names)), device=device, dtype=torch.float32)\n",
    "    z = torch.randn(B, z_dim, device=device)\n",
    "    gen_imgs_int8 = G_int8(z, rand_attrs)  # [-1,1], shape (B,3,128,128)\n",
    "print(\"INT8 gen random:\", tuple(gen_imgs_int8.shape), gen_imgs_int8.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b44088",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 14: Save quantized models\n",
    "ckpt_dir = base_dir / \"checkpoints_qat\"\n",
    "ckpt_dir.mkdir(exist_ok=True)\n",
    "torch.save(G.state_dict(), ckpt_dir / \"G_qat_int8.pt\")\n",
    "torch.save(D.state_dict(), ckpt_dir / \"D_qat_int8.pt\")\n",
    "print(\"Saved INT8 checkpoints to:\", ckpt_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ab647",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 15: Free-form attribute input -> generate (no saving)\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def to_display(img_t):  # (3,H,W) in [-1,1] -> (H,W,3) in [0,1] for imshow\n",
    "    return ((img_t.clamp(-1,1) + 1) / 2.0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "# Build a name->index map once\n",
    "name_to_idx = {n.lower(): i for i, n in enumerate(attr_names)}\n",
    "\n",
    "def parse_freeform_attrs(s: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Parse a free-form string like:\n",
    "    - 'Smiling, Blond_Hair, -Male, Young'\n",
    "    - '+Smiling +Blond_Hair -Male +Young'\n",
    "    - 'Smiling Blond_Hair !Male Young'\n",
    "    Returns a (attr_dim,) float tensor with 0/1 values.\n",
    "    Unspecified attributes default to 0.\n",
    "    \"\"\"\n",
    "    v = torch.zeros(len(attr_names), device=device)\n",
    "    if not s or not s.strip():\n",
    "        return v\n",
    "    # Split on comma or whitespace\n",
    "    tokens = re.split(r\"[,\\s]+\", s.strip())\n",
    "    for tok in tokens:\n",
    "        if not tok:\n",
    "            continue\n",
    "        sign = +1\n",
    "        raw = tok\n",
    "        if tok in \"+-!\":\n",
    "            sign = -1 if tok in \"-!\" else +1\n",
    "            raw = tok[1:]\n",
    "        key = raw.strip().lower()\n",
    "        if key in name_to_idx:\n",
    "            v[name_to_idx[key]] = 1.0 if sign > 0 else 0.0\n",
    "    return v\n",
    "\n",
    "# --- Interactive-like usage in a script/notebook ---\n",
    "# Provide any free-form string here:\n",
    "user_str = \"Smiling, Blond_Hair, -Male, Young\"  # edit this string freely\n",
    "\n",
    "# Prepare batch attrs by repeating parsed vector\n",
    "with torch.inference_mode():\n",
    "    attr_vec = parse_freeform_attrs(user_str).unsqueeze(0)  # (1, attr_dim)\n",
    "    B = 8\n",
    "    attrs_batch = attr_vec.repeat(B, 1)                     # (B, attr_dim)\n",
    "    z = torch.randn(B, z_dim, device=device)\n",
    "\n",
    "    # Use the current G in memory (INT8 after Cell 12, or FP32 if placed after Cell 9)\n",
    "    G.eval()\n",
    "    gen_imgs = G(z, attrs_batch)  # [-1,1], shape (B,3,128,128)\n",
    "\n",
    "# Visualize first 4 samples inline\n",
    "plt.figure(figsize=(8,8))\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.imshow(to_display(gen_imgs[i]))\n",
    "    plt.title(user_str)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Generated shape:\", tuple(gen_imgs.shape), \"dtype:\", gen_imgs.dtype)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
